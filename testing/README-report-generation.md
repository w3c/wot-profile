# Implementation Report

This directory contains the
[WoT Profile Implementation report](https://w3c.github.io/wot-profile/testing/report.html)
and the sources for it.
The report itself is in [report.html](report.html).
Do not edit this file,
it is generated by executing a script
and any edits would be overwritten the next time you generate it.

## Suppressing Assertions
The file
[suppressed.csv](suppressed.csv)
can be used to list assertions for which
test results should be ignored.  Such results will also not be used for
"roll-up" results (when a child assertion, one with an underscore in its
name, is used to break down assertions with multiple options into simpler,
separately testable assertions).

## Manual Assertions
Assertions listed in
[manual.csv](manual.csv)
do not have automatic tests and need
manual testing or declarations.

## At-Risk Highlighting
The file
[atrisk.css](atrisk.css) provides highlighting for at-risk elements in
the report but should NOT be edited directly as it is autogenerated.
Instead edit 
[inputs/atrisk.csv](inputs/atrisk.csv) if you need to force additional assertions to be
marked as at-risk.

## Report Template 
The HTML template for the Implementation Report is in 
[inputs/templates/report.html](inputs/templates/report.html).
If you wish to edit the main explanatory text of the report or
update the metadata (date, authors, etc.), do so here.

## Implementation Descriptions
Implementations are described in HTML files in
[inputs/implementations](inputs/implementations), with one
file per implementation.
The IDs declared in these descriptions should be unique and 
descriptive as they will be used elsewhere, for example, 
in the interoperability data files referring to those implementations.
To add a new description, use the template in 
[inputs/templates/impldesc.html](inputs/templates/impldesc.html).

Implementations should also be entered into the table
in [inputs/impl.csv](inputs/impl.csv) with identifiers, titles, and roles
consistent with those assigned in the above HTML files.

## Test Specifications
A procedure for testing each normative assertion should be given in
[inputs/testspec.html](inputs/testspec.html).
These may be included as an appendix in the report,
but are currently suppressed.

Test specifications can be given both for
assertions given in the specification 
(see [inputs/template.csv](inputs/template.csv) 
for an automatically
generated list of identifiers) and for any extra assertions in 
[inputs/extra-asserts.html](inputs/extra-asserts.html).

## Extra Assertions
Assertions used for testing but not (yet) included in the specification
may be listed in 
[inputs/extra-asserts.html](inputs/extra-asserts.html).
The intention is that these assertions should
eventually, and before final release, be inserted into the final specification.

## At-Risk Assertions
Assertions related to features that are at risk of being deleted from the final
CR should be identified in the [inputs/atrisk.csv](inputs/atrisk.csv) file.
The assertion text for these will be given a special color in the report table.
Assertions for which there are not enough passing tests will also be highlighted 
automatically, this input file is for additional features that need to be 
marked as at-risk for other reasons.

## Categories
Assertions can be assigned to a category in
[inputs/categories.csv](inputs/categories.csv).
Eventually the table _may_ be sorted to group categories
into sections; for now it is just an extra column.

## Dependencies
Dependencies between assertions can be recorded in
[inputs/depends.csv](inputs/depends.csv).  
The "Parents" column relates detailed assertions to more general assertions.
the "Contexts" column indicates assertions that only need to be considered in
a particular context (either syntactic, if pointing at another vocabulary item,
or logical, if pointing at another optional assertion).
Both "Parents" and "Contexts" may have multiple items separated by spaces.
Entries should be IDs of other assertions.
Use `"null"` if there is no dependency.

## Result Data
Each implementation should record
which features they have implemented and tested under the 
[inputs/results](inputs/results) directory.
All data will be read and merged into the report.
Mark each implemented feature with a status of either 
`"pass"` (if it satisfies the specification)
or 
`"fail"` (if it does not).  
If you have not implemented a feature, list its status as `"not-impl"`.

Features not listed will not be included in the sums; this is 
distinct from `"not-impl"` as absence of a feature is meant to be used
to allow different features to be reported in different files.

If you did not implement a feature on purpose please indicate this explicitly.

Any other status will be ignored (e.g., `"null"` as used in the template).
If you have tested a feature in multiple
implementations check in one file per implementation, using as a filename
the id given in the template for the implementations' description.
Use a convention
like `ORG-IMPL-MODULE.csv` for the filename.
The filename should also be used as an id in the 
description of each implementation.

The [template.csv](template.csv) 
file lists all assertions but with a `"null"` status.
*Do not edit this file; it is autogenerated.*
It is provided so
you can use it as a reference and as a basis for your own data files.

Files should be in CSV format, including headers as defined in 
[template.csv](template.csv),
and will be parsed by the `csvtojson` Node.js library.


